# -*- coding: utf-8 -*-
"""Document-Aware Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qf_5XOYJC0zu44j8qdSlQlkh9wiUlT2A
"""

!pip install -q langchain groq llama-index chromadb sentence-transformers pypdf PyPDF2 langchain-community

from google.colab import userdata
import os
os.environ["GROQ_API_KEY"] = userdata.get('groq_api')

"""#Step 1: Choose a Document & make chunks of it
Document parsing: PDF, HTML, TXT
"""

from langchain.document_loaders import PyPDFLoader

pdf_path = "/content/DL-book.pdf"
loader = PyPDFLoader(pdf_path)
docs = loader.load()

len(docs)

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Split document into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
doc = splitter.split_documents(docs)
print(f'No. of chunks {len(doc)}')

"""#Step 2 : Embedding the chunks & storing it in ChromaDB/ Vector DataBase"""

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import shutil

embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')

vector_store = Chroma.from_documents(documents = doc, embedding=embedding_model, persist_directory="./dl_embeddings")

vector_store.persist()  #This saves the vectors and we can reuse the vector_store later
# retriever = vector_store.as_retriever()  #This converts your vector store into a retriever interface

shutil.make_archive('dl_embeddings','zip','./dl_embeddings')

"""#Step 3: Using Groq AI
- Groq's ultra-fast LLM inference
- Efficient handling of long-context documents
- used Groq's model(llama-3.3-70b-versatile)
"""

from langchain.llms import Groq   #cannot import name 'Groq' from 'langchain.llms"

pip install langchain groqcloud langchain-groq

from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA

os.environ["GROQ_API_KEY"] = userdata.get('groq_api')

llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.2)

qa_chain = RetrievalQA.from_chain_type(
    llm= llm,
    retriever= vector_store.as_retriever(search_kwargs={"k": 5}),
    return_source_documents= True)

"""#Step 4: Making a ChatBOT (quries & responses)


*   Giving a Question and cheching for the response


"""

query = 'Explain what is overfitting in deep learning.'
response = qa_chain(query)

from IPython.display import Markdown
Markdown(response["result"])

query2 = "Explain what is vanishing gradient in deep learning."
response2 = qa_chain(query2)

Markdown(response2["result"])

"""#Showing the Source references of the response"""

print(response['result'])
for doc in response['source_documents']:
    print("\nSource Page:","-", doc.metadata.get("page"))

"""#Multi-turn conversation with RetrievalQA :


*   It gives the response remembering past interactions


"""

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
multi_chain = ConversationalRetrievalChain.from_llm(llm=llm,retriever=vector_store.as_retriever(),memory=memory)

q1 = "What is an Activation Function?"
response1 = multi_chain.run(q1)
Markdown(response1)

"""###Here's an another query which gives the response based on previous query, Here i haven't mentioned any activation function."""

q2 = "What are the types of it?"  #This gives the response based on previous query, Here i haven't mentioned any activation function.
response2 = multi_chain.run(q2)
Markdown(response2)

"""#Handling Ambigous/Unknown queries"""

q3 = "What is PowerBI?"
response3 = multi_chain.run(q3)
Markdown(response3)

q4 = "What is an High rise building & explain types of it?"
response4 = multi_chain.run(q4)
Markdown(response4)

